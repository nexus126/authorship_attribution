{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYdqwvZ_soRI"
   },
   "outputs": [],
   "source": [
    "__author__   = \"Federico Ranaldi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-actwFfKs1dh"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "37ke4cOIwG_d",
    "outputId": "c83af90e-5b9d-4e49-cd71-643a7e831917"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POzTYrZcGiGY",
    "outputId": "650a2c43-65c3-49fe-a7e0-71bab6c17e95"
   },
   "outputs": [],
   "source": [
    "df[\"author\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "PaXL3IgwGuhe",
    "outputId": "0f8a0fa1-1220-469b-8b35-d28e2a050b0b"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"author\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoqH63smGsqg"
   },
   "outputs": [],
   "source": [
    "#df=df[df[\"author\"].isin([\"Mary Wollstonecraft Shelley\",\"Howard Phillips Lovecraft\",\"Edgar Allan Poe\"])]\n",
    "df=df[df[\"author\"].isin([\"Mary Wollstonecraft Shelley\",\"Howard Phillips Lovecraft\",\"Edgar Allan Poe\",\"William Shakespeare\",\"Percy Bysshe Shelley\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57CfRB_xITkG"
   },
   "outputs": [],
   "source": [
    "df.index=range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "_h-_MqRSzCk7",
    "outputId": "06f49e1a-7d52-4d2a-b422-a7c477ffa01f"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"author\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e39XACpO6aOK"
   },
   "source": [
    "**WORD-TOKENIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8BMfSXL7xp0",
    "outputId": "175fdfd9-9ec8-4bff-d0fc-a71a8d50a1ee"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAkTVaIo7mSG"
   },
   "source": [
    "**POS-TAGGIN(PART-OF-SPEECH TAGGING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCn_4UcV6Zca",
    "outputId": "8cd28f22-d2e7-4671-d384-ef575196a697"
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "df['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7zvERFQ8x4v",
    "outputId": "bc18c0d9-bbe1-412b-f516-b06ea3473f1a"
   },
   "outputs": [],
   "source": [
    "df['tokenized_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSEc_f1a9vRK"
   },
   "source": [
    "**TO-LOWER-CASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Opduytm9utQ"
   },
   "outputs": [],
   "source": [
    "def listOfLists(L):\n",
    "  newL=[]\n",
    "  for t in L:\n",
    "    newL.append(list(t))\n",
    "  return newL\n",
    "\n",
    "def toLowerCase(L):\n",
    "  for l in L:\n",
    "    l[0]=l[0].lower()\n",
    "  return L\n",
    "\n",
    "#trasformazione da lista di tuple a lista di liste\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: listOfLists(x))\n",
    "\n",
    "#trasformazione dei token in lower-case\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: toLowerCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kt5II08RVUtT",
    "outputId": "adaf6f5c-7095-4ec9-b5ab-e5ba761f0bef"
   },
   "outputs": [],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwcR1woAWPFH"
   },
   "source": [
    "**STOPWORDS-REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWJj2Qz6WNoy",
    "outputId": "7331e12c-1526-4cd5-ed38-f97c6d560d0e"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_list = stopwords.words('english')+list(string.punctuation)+[\" \"]+[\"\"] #noise removal:insieme alle stopwords viene eliminata la punteggiatura\n",
    "\n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqZ1vC_SPmKQ"
   },
   "outputs": [],
   "source": [
    "#RIMOZIONE DELLE STOPWORDS (comprende rimozione dei rumori)\n",
    "#rimuove troppe parole!!!\n",
    "#rimuove gli elementi della sottolista b dagli elementi della sottolista a\n",
    "def removeSublist(a,b):\n",
    "  for el in b:\n",
    "    a.remove(el)\n",
    "\n",
    "#df['tokenized_sents'].apply(lambda x: removeSublist(x,[couple for couple in x if not(set(couple[0]).isdisjoint(stop_list))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM2IUUx3dTV3"
   },
   "source": [
    "**NOISE-REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFB5IuSHKu95",
    "outputId": "d94fdd31-d458-4ddc-bb2e-abab4e60850c"
   },
   "outputs": [],
   "source": [
    "#solo rimozione di rumori(punteggiatura e spazi vuoti)\n",
    "\n",
    "def removeSublist(a,b):\n",
    "  for el in b:\n",
    "    a.remove(el)\n",
    "\n",
    "noises_list=list(string.punctuation)+[\" \"]+[\"\"]\n",
    "\n",
    "df['tokenized_sents'].apply(lambda x: removeSublist(x,[couple for couple in x if not(set(couple[0]).isdisjoint(noises_list))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zssr59m8YSZ5",
    "outputId": "e117a88b-4413-469d-82cf-39315c0a2524"
   },
   "outputs": [],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaGZjTHNd5-P"
   },
   "source": [
    "**LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lz2jito8vkrn"
   },
   "outputs": [],
   "source": [
    "#Lemmatizzazione in base al Pos-Tag\n",
    "#Attenzione funziona solo con #codice per POS-TAG\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6aU03zeaxdUJ",
    "outputId": "4a3fbeb9-e47d-4e5c-8fde-3b2dd6461067"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos=get_wordnet_pos(\"VBZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RR6BMrDvd9Oj"
   },
   "outputs": [],
   "source": [
    "#Lemmatizzazione in base al Pos-Tag\n",
    "#Attenzione funziona solo con #codice per POS-TAG\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizeToken(L):\n",
    "  for i in range(len(L)):\n",
    "    if(get_wordnet_pos(L[i][1])!=0):\n",
    "      L[i][0]=lemmatizer.lemmatize(L[i][0],pos=get_wordnet_pos(L[i][1]))\n",
    "  return L\n",
    "\n",
    "df[\"tokenized_sents\"]=df['tokenized_sents'].apply(lambda x: lemmatizeToken(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_FFLgbEsmUB",
    "outputId": "37012210-f551-4dec-c666-d4e7ee5c8045"
   },
   "outputs": [],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1spi9dj_duy4"
   },
   "source": [
    "CONCATENAZIONE DELLE COPPIE TOKEN-TAG OPPURE TOKEN-ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_BbodVWEEfl"
   },
   "outputs": [],
   "source": [
    "\n",
    "df[\"tokenized_sents\"]=df[\"tokenized_sents\"].apply(lambda x: ' '.join([el[0]+el[1] for el in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5A26nOYGeJ3"
   },
   "outputs": [],
   "source": [
    "df.index=range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Q9skfLNSIMOX",
    "outputId": "329a87d4-43bd-40d4-ade7-4b2753eabe55"
   },
   "outputs": [],
   "source": [
    "df.loc[970,\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zATjF353t2JA"
   },
   "source": [
    "# **VETTORIZZAZIONE CON TF-IDF**\n",
    "\n",
    "Il TF-IDF è un algoritmo di vettorizzazione che converte in un formato numerico il nostro corpus facendo emergere termini specifici, pesando diversamente i termini molto rari o molto comuni in modo da assegnare loro un punteggio basso.\n",
    "TF sta per term frequency, mentre IDF sta per inverse document frequency. \n",
    "Il valore TF-IDF legato ad una parola(o token) aumenta proporzionalmente al numero di volte che questa appare nel documento ed è compensato dal numero di documenti nel corpus che la contengono.Se una parola è contenuta in molti documenti allora è probabile che quella non sia una parola altamente specifica per il documento.\n",
    "Nell'algorimo TF-IDF così come in BOW non viene codificato l'ordine dei token all'interno di un testo e ciò causa un ulteriore perdita di informazione oltre a quella generata dal preprocessing.\n",
    "Mentre BOW si limita a codificare le parole contenute in un testo preprocessato TFIDF codifica l'informazione che descrive l'importanza di una parola all'interno di un testo che fa parte di un insieme di documenti su cui il modello verrà addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHHuFcYjE3Zr"
   },
   "outputs": [],
   "source": [
    "# inizializziamo il vettorizzatore\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "# fit_transform applica il TF-IDF ai testi puliti - salviamo la matrice di vettori in X\n",
    "data_vectorized = vectorizer.fit_transform(df['tokenized_sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_pG6K7oG5kG",
    "outputId": "7d7e980d-b5b0-422d-ec78-81ff8d45adcc"
   },
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQZbtCNkuB6c",
    "outputId": "c3b7daf3-6f17-4c14-b076-bae823e519a8"
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV4TTGXwuF6d"
   },
   "outputs": [],
   "source": [
    "X=data_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qzm7y66AG-jp",
    "outputId": "fb2f3a63-c1fb-4ad7-bd69-1b7e5a0d11fc"
   },
   "outputs": [],
   "source": [
    "Y=np.empty((len(df),1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RayIHv0dG-g2",
    "outputId": "0071d788-828c-4103-a337-97cf9608eccc"
   },
   "outputs": [],
   "source": [
    "#trasformazione delle classi targhet da stringhe a numeri\n",
    "authors=df[\"author\"].unique() #lista/insieme degli autori\n",
    "\n",
    "targets=np.array(df[\"author\"])\n",
    "\n",
    "for i in range (len(df)):\n",
    "  Y[i]=np.where(authors == targets[i])[0][0]\n",
    "\n",
    "Y=Y.astype(\"int\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY2c08JsHFFr",
    "outputId": "f291e97e-34f0-4174-a437-825321d1fe18"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT8_ex-EHFC_",
    "outputId": "efc980fe-ca11-44fe-9c9b-e26396573eb8"
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdgkPpGtHMVr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl0jUIS_HMS_",
    "outputId": "11125352-a4cb-49b7-9596-9a75eab92cdc"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-wT99q3HQ2U",
    "outputId": "a747fca7-69ed-490f-a527-5a5ed7949714"
   },
   "outputs": [],
   "source": [
    "print(\"Xtrain\",type(X_train))\n",
    "print(\"Ytrain\",type(Y_train))\n",
    "print(\"Xtest\",type(X_test))\n",
    "print(\"Ytest\",type(Y_test))\n",
    "print(\"Xtrain\",X_train.dtype)\n",
    "print(\"Ytrain\",Y_train.dtype)\n",
    "print(\"Xtest\",X_test.dtype)\n",
    "print(\"Ytest\",Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_3geQeGHbVM"
   },
   "source": [
    "**Trasformazione degli array numpy in tensori pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCmVrw8KHQxe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_test = torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJEYPSVzHhWi",
    "outputId": "5321e9ab-97d4-49dc-dc9d-daee498b1336"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xGLtIXUHoc7"
   },
   "source": [
    "# **Creazione della rete neurale feed-forward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qzapsGdHt98",
    "outputId": "5ba8fdbc-b528-469e-c627-6233b68f7149"
   },
   "outputs": [],
   "source": [
    "#il numero di neuroni viene fatto corrispondere al numero di colonne di un vettore rappresentante un dato\n",
    "#cioè in base al numero di termini che hanno una tfidf maggiore di 0\n",
    "\n",
    "len(X_train[0])\n",
    "\n",
    "input_layer_neurons=len(X_train[0])\n",
    "output_layer_neurons=5\n",
    "\n",
    "print(input_layer_neurons)\n",
    "print(output_layer_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wYnwlkxHlOa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "#creazione della classe Net\n",
    "#Net è una sottoclasse di nn.Module quindi eredita tutti i suoi metodi e attributi\n",
    "class Net(nn.Module):\n",
    "  def __init__(self): #costruttore della classe\n",
    "    super().__init__()  #viene richiamato il costruttore della classe nn.Module\n",
    "\n",
    "    #definizione dei layers(quindi degli strati della rete che avranno un numero di ingressi e un numero di uscite determinati)\n",
    "    #nn.linear applica una trasformazione lineare ai dati\n",
    "    self.fc1=nn.Linear(input_layer_neurons,124)  #Input Layer : ha 28*28 ingressi(quindi 784 neuroni) e 64 uscite possibili(cioé gli ingressi del layer successivo saranno 64)\n",
    "    self.fc2=nn.Linear(124,64)     #Hidden Layer 1: ha 64 ingressi e 64 uscite\n",
    "    #self.fc3=nn.Linear(64,64)     #Hidden Layer 2: ha 64 ingressi e 64 uscite\n",
    "    self.fc4=nn.Linear(64,output_layer_neurons)     #Output Layer : ha 64 ingressi e 10 uscite perché le classi possibili sono 10\n",
    "  \n",
    "  #definizione della logica secondo cui i dati attraversano i vari layer\n",
    "  #ad ogni dato quando attraversa ogni neurone viene applicata la funzione relu\n",
    "  def forward(self,x):\n",
    "    x=fn.relu(self.fc1(x))\n",
    "    x=fn.relu(self.fc2(x))\n",
    "    #x=fn.relu(self.fc3(x))\n",
    "    x=self.fc4(x)\n",
    "\n",
    "    #return x\n",
    "    return fn.log_softmax(x,dim=1)  #ritorna la distribuzione probabilistica dell'output della rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxc-YmUGHkuO"
   },
   "outputs": [],
   "source": [
    "net=Net() #istanziazione della rete neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwQM6EmcICLF"
   },
   "source": [
    "# Caricamento dei dati(comprende suddivisione in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr9vWHjHICxz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_data = TensorDataset(X_train,Y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test,Y_test)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBRPt0C2H8y-"
   },
   "source": [
    "# FASE DI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yRCFPRSH7-K",
    "outputId": "0f42479a-cc54-4392-a0ed-175e33d0dd5b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"gpu available\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZYjiOFsIO1X",
    "outputId": "a80ea868-d69c-469a-9e89-ee3345bb9881"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#implementazione della loss-function o funzione di costo\n",
    "#la loss-function calcola quanto gli output della rete neurale si discostano dai valori reali\n",
    "#il risultato di una loss-function può essere uno scalare(entropia incrociata...) oppure un vettore(one-hot array)\n",
    "\n",
    "loss_function=nn.CrossEntropyLoss();\n",
    "\n",
    "#implementazione dell'ottimizzatore \n",
    "#l'ottimizzatore si occupa di modificare i pesi dei collegamenti della rete in modo che questa restituisca un output che si avvicini il più possibile al valore corretto\n",
    "#net.parameters() restituisce un tensore che rappresenta i parametri della rete ossia tutti i pesi della rete\n",
    "#il learning rate (lr) è un valore che dice di quanto devono essere aggiustati i pesi a ogni iterazione\n",
    "#Adam è un ottimizzatore che implementa la discesa stocastica del gradiente (Stochastic Gradient Descent)\n",
    "\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "#l'epoch è rappresenta l'intera fase in cui al modello sono stati passati tutti i dati del trainset suddivisi in batches\n",
    "#se si effettuano poche epoche il modello non si addestra bene o meglio non aggiusta i propri pesi nel modo migliore\n",
    "#se si effettuano troppe epoche il modello rischia l'overfitting (si adegua troppo al training set e perde la capacità di generalizzare)\n",
    "\n",
    "for epoch in range(3):\n",
    "  for data in train_data:\n",
    "    X, y = data #suddividiamo l'esempio in features (X) e classe di appartenenza (y)\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "    net.zero_grad() #setta il gradiente a 0\n",
    "    output=net(X.view(-1,input_layer_neurons))  #ad ogni passo alla rete viene fornito come input l'ouput dell'iterazione precedente\n",
    "    #print(output)\n",
    "    loss=fn.nll_loss(output,y.long())  #calcola la loss-function sull'output della rete al passo corrente rispetto al target reale\n",
    "    loss.backward() #l'errore commesso viene propagato verso ogni collegamento dallo strato di output agli strati più interni(back-propagation)\n",
    "    optimizer.step()  #basandosi sui valori riportati dal gradiente effettua l'aggiustamento dei parametri della rete in maniera opportuna  \n",
    "  print(loss) #stampa la loss function dopo ogni epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atp3HWRoIOxw",
    "outputId": "0284b24c-fb0f-48e3-e50a-edb1d2d66a0a"
   },
   "outputs": [],
   "source": [
    "correct=0 #numero di esempi su cui la rete ha predetto in maniera corretta\n",
    "total=0 #numero di esempi su cui la rete ha effettuato una predizione\n",
    "\n",
    "with torch.no_grad(): #nella fase di test non dobbiamo aggiustare i pesi o calcolare la loss function percui non ci interessa il gradiente\n",
    "  for data in test_data:\n",
    "    X, y = data\n",
    "    output=net(X.view(-1,input_layer_neurons))\n",
    "    #print(y)\n",
    "    for idx, i in enumerate(output):\n",
    "      #print(torch.argmax(i),y[idx])\n",
    "      if(torch.argmax(i)==y[idx]):\n",
    "        correct+=1\n",
    "      total+=1\n",
    "\n",
    "print(\"ACCURACY: \",round(correct/total,3)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd30By9lSDdB"
   },
   "source": [
    "10- 61.19,61.8-----62.8%\n",
    "3- 77.5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AUTHOR_REC_POS_TFIDF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
